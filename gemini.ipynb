{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a426d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5c3a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyDmh4QOgBpcb23e-Ckfvl692Ci71ze7J2Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1617693",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_PATH=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e88185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2424b9d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTelugu\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTelugu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Filter out invalid rows\u001b[39;00m\n\u001b[0;32m     14\u001b[0m valid_rows \u001b[38;5;241m=\u001b[39m df[\n\u001b[1;32m---> 15\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna() \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     16\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTelugu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna() \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     17\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     18\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTelugu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;241m~\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTelugu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.*google\u001b[39m\u001b[38;5;124m\"\u001b[39m, case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m ]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Convert to dictionary\u001b[39;00m\n\u001b[0;32m     23\u001b[0m glossary_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(valid_rows[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m\"\u001b[39m], valid_rows[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTelugu\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\vinay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1530\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"gloss.xlsx\")  # Replace with your file\n",
    "\n",
    "# Rename columns explicitly for clarity (assumes 2 columns: English, Telugu\n",
    "\n",
    "# Clean whitespace\n",
    "df[\"English\"] = df[\"English\"].astype(str).str.strip()\n",
    "df[\"Telugu\"] = df[\"Telugu\"].astype(str).str.strip()\n",
    "\n",
    "# Filter out invalid rows\n",
    "valid_rows = df[\n",
    "    df[\"English\"].notna() &\n",
    "    df[\"Telugu\"].notna() &\n",
    "    df[\"English\"].str.len() > 0 &\n",
    "    df[\"Telugu\"].str.len() > 0 &\n",
    "    ~df[\"Telugu\"].str.contains(r\"http.*google\", case=False, na=False)\n",
    "]\n",
    "\n",
    "# Convert to dictionary\n",
    "glossary_dict = dict(zip(valid_rows[\"English\"], valid_rows[\"Telugu\"]))\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"glossary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(glossary_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ glossary.json created with {len(glossary_dict)} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb34501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ glossary.json created with 993 entries.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"gloss.xlsx\")  # Update path if needed\n",
    "\n",
    "# Rename columns\n",
    "# df.columns = [\"English\", \"Telugu\"]\n",
    "\n",
    "# Strip whitespace and ensure string type\n",
    "df[\"English\"] = df[\"English\"].astype(str).str.strip()\n",
    "df[\"Telugu\"] = df[\"Telugu\"].astype(str).str.strip()\n",
    "\n",
    "# Filter valid rows using parentheses around each condition\n",
    "valid_rows = df[\n",
    "    (df[\"English\"].notna()) &\n",
    "    (df[\"Telugu\"].notna()) &\n",
    "    (df[\"English\"].str.len() > 0) &\n",
    "    (df[\"Telugu\"].str.len() > 0) &\n",
    "    (~df[\"Telugu\"].str.contains(r\"http.*google\", case=False, na=False))\n",
    "]\n",
    "\n",
    "# Convert to dictionary\n",
    "glossary_dict = dict(zip(valid_rows[\"English\"], valid_rows[\"Telugu\"]))\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"glossary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(glossary_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ glossary.json created with {len(glossary_dict)} entries.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
